# -*- coding: utf-8 -*-
"""Copy_of_Untitled18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ab2Ru_yzsrL4NuuBSIZ9XRoAqIYqjH_A
"""

import pandas as pd
import numpy as np

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
# sample_result = pd.read_csv('sample_submission.csv')

uni_df_train = list(train.var13.unique())
uni_df_train.remove(np.nan)
df_count = train.groupby('var13')
for i in uni_df_train:
  # print(i)
  df = df_count.get_group(i)
  count = df.shape[0]
  train['var13'].replace(i,count,inplace=True)

# uni_df_train3 = list(train.var1.unique())
# df_count_3 = train.groupby('var1')
# print(uni_df_train3)
# for i in uni_df_train3:
#   df_3 = df_count_3.get_group(i)
#   count_3 = df_3.shape[0]
#   train.var1.replace(i,count_3, inplace=True)

# train.drop('var1',axis=1,inplace=True)

uni_df_train_2 = list(train.var15.unique())
df_count_2 = train.groupby('var15')
print(uni_df_train_2)
for i in uni_df_train_2:
  # print(i)
  df_2 = df_count_2.get_group(i)
  count_2 = df_2.shape[0]
  train['var15'].replace(i,count_2,inplace=True)

train

from sklearn.impute import KNNImputer
impute = KNNImputer(n_neighbors=2)
train = impute.fit_transform(train)
# train = pd.read_csv('train_n.csv')
# train

train = pd.DataFrame(train)
train.columns = ['ID','var1','var2', 'var3', 'var4', 'var5', 'var6', 'var7', 'var8', 'var9', 'var10', 'var11', 'var12', 'var13','var14','var15','target']

# cols = ['ID','var2', 'var3', 'var4', 'var5', 'var6', 'var7', 'var8', 'var9', 'var10', 'var11', 'var12', 'var13','var14','var15','target']
# train.columns = cols
# train

# train.to_csv('train_n.csv',index=False)

y_train = train.target
x_train = train.drop('target',axis=1)
# cols = x_train.columns

x_train.drop('ID',axis=1,inplace=True)

# from sklearn.preprocessing import StandardScaler
# scale = StandardScaler()
# x_train_scale = pd.DataFrame(scale.fit_transform(x_train))
# x_train_scale.columns = cols
# x_train_scale.drop('ID',axis=1,inplace=True)

# x = np.nan
# r = 6
# l = [1,2,3,4,5,6,x,23,8]
# l.remove(6)
# l

test

# test.columns = ['ID','var2', 'var3', 'var4', 'var5', 'var6', 'var7', 'var8', 'var9', 'var10', 'var11', 'var12', 'var13','var14','var15']
test

cols = list(test.columns)
x = np.nan

uni_df_test = list(test.var13.unique())
df_count = test.groupby('var13')
uni_df_test.pop(-5)
for i in uni_df_test:
  # print(i)
  df = df_count.get_group(i)
  count = df.shape[0]
  test['var13'].replace(i,count,inplace=True)

# uni_df_test3 = list(test.var1.unique())
# df_count_3 = test.groupby('var1')
# for i in uni_df_test3:
#   df_3 = df_count_3.get_group(i)
#   count_3 = df_3.shape[0]
#   test.var1.replace(i,count_3, inplace=True)
# test.drop('var1',axis=1,inplace=True)
# cols.remove('var1')

uni_df_test_2 =list(test.var15.unique())
df_count_2 = test.groupby('var15')
for i in uni_df_test_2:
  # print(i)
  df_2 = df_count_2.get_group(i)
  count_2 = df_2.shape[0]
  test['var15'].replace(i,count_2,inplace=True)

from sklearn.impute import KNNImputer
impute = KNNImputer(n_neighbors=2)
test = impute.fit_transform(test)
# test = pd.DataFrame(test)
# test.columns = cols

print(test)

# from sklearn.preprocessing import StandardScaler
# scale = StandardScaler()
# x_test_scale = pd.DataFrame(scale.fit_transform(test))
# x_test_scale.columns = cols
# x_test_scale.drop('ID',axis=1,inplace=True)
# x_test_scale
# y_test = x_test_scale.target

# x_test_scale.columns = ['ID','var2', 'var3', 'var4', 'var5', 'var6', 'var7', 'var8', 'var9', 'var10', 'var11', 'var12', 'var13','var14','var15']

x_test = pd.DataFrame(test)
x_test.columns = ['ID','var1','var2', 'var3', 'var4', 'var5', 'var6', 'var7', 'var8', 'var9', 'var10', 'var11', 'var12', 'var13','var14','var15']

x_test.drop('ID',axis=1,inplace=True)

# from sklearn.preprocessing import PolynomialFeatures

# eq = PolynomialFeatures(degree=3)
# poly_x_train_scale = eq.fit_transform(x_train_scale)
# poly_x_test_scale = eq.fit_transform(x_test_scale)

# from sklearn.linear_model import LinearRegression
# model = LinearRegression()
# model.fit(poly_x_train_scale,y_train)

# pd.DataFrame(poly_x_train_scale)
# pd.DataFrame(poly_x_test_scale)

# train.drop('ID',axis=1,inplace=True)
# train

# x_train = np.array(train).reshape(-1,1)

from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.03, max_depth=5, random_state=82)
model.fit(x_train,y_train)

# import numpy as np
# from keras.models import Sequential
# from keras.layers import Dense

# # Define the deep learning model
# def build_model(input_dim):
#     model = Sequential()
#     model.add(Dense(128, activation='relu', input_dim=input_dim))
#     model.add(Dense(64, activation='relu'))
#     model.add(Dense(1, activation='linear'))  # For regression tasks, use 'linear' activation

#     model.compile(optimizer='adam', loss='mean_squared_error')  # Use appropriate loss for your task

#     return model

# # Assuming x_train has shape (300000, 15) and y_train has shape (300000,)
# input_dim = x_train.shape[1]
# model = build_model(input_dim)

# # Train the model
# model.fit(x_train, y_train, epochs=20, batch_size=12)  # You can adjust epochs and batch_size

x_train

prdct = model.predict(x_test)

df_prediction = pd.DataFrame(prdct)

list_cols = ['Target']
df_prediction.columns = list_cols

df_prediction
df_prediction.to_csv('/content/output.csv',index=True)

df_prediction